{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss, MultiHorizonMetric\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Refined_Data/Grouped_Data/Input_Data2.csv\"\r\n",
    "\r\n",
    "group = [\r\n",
    "    [4],[11, 12],[34],[40],[10],[42],[41]                                   # 0~ 6\r\n",
    "    ,[6, 8, 13, 17, 26, 48, 53, 55, 56],[7, 18],[27, 57],[35, 46, 47],[25]  # 7 ~ 11\r\n",
    "    ,[1, 31],[9, 32],[3],[33]                                               # 12 ~ 15\r\n",
    "    ,[2, 14, 22, 37, 44, 52, 54],[15],[38, 58, 43],[29, 39],[45],[23]       # 16 ~ 21\r\n",
    "    ,[5],[16, 24],[19, 20, 21, 49, 50, 51],[28, 36, 60],[59], [30]          # 22 ~ 27\r\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, parse_dates = [\"date_time\"])\r\n",
    "\r\n",
    "data['num']     =   data['num'].apply(str)\r\n",
    "data['day_of_Week']     =   data['day_of_Week'].apply(str)\r\n",
    "data['day_of_month']    =   data['day_of_month'].apply(str)\r\n",
    "data['24Hour']  =   data['24Hour'].apply(str)\r\n",
    "data['holiday'] =   data['holiday'].apply(str)\r\n",
    "data['Weekend'] =   data['Weekend'].apply(str)\r\n",
    "data['energy_group'] = data['energy_group'].apply(str)\r\n",
    "data['hour_cat']=   data['hour_cat'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bag = [data.loc[data[\"energy_group\"] == str(i)].copy() for i in range(len(group))]\r\n",
    "\r\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "in_loss_step=0.0355, train_loss_epoch=0.0344]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|██████████| 11/11 [00:06<00:00,  1.64it/s, loss=0.0346, v_num=16, val_loss=0.0501, train_loss_step=0.0349, train_loss_epoch=0.0348]\n",
      "Epoch 26:  91%|█████████ | 10/11 [00:06<00:00,  1.56it/s, loss=0.0335, v_num=16, val_loss=0.0501, train_loss_step=0.0321, train_loss_epoch=0.0348]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s, loss=0.0335, v_num=16, val_loss=0.0508, train_loss_step=0.0356, train_loss_epoch=0.0323]\n",
      "Epoch 27:  91%|█████████ | 10/11 [00:06<00:00,  1.55it/s, loss=0.0319, v_num=16, val_loss=0.0508, train_loss_step=0.0343, train_loss_epoch=0.0323]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|██████████| 11/11 [00:06<00:00,  1.66it/s, loss=0.0319, v_num=16, val_loss=0.048, train_loss_step=0.0332, train_loss_epoch=0.0316] \n",
      "Epoch 28:  91%|█████████ | 10/11 [00:06<00:00,  1.57it/s, loss=0.0328, v_num=16, val_loss=0.048, train_loss_step=0.0335, train_loss_epoch=0.0316]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|██████████| 11/11 [00:06<00:00,  1.69it/s, loss=0.0328, v_num=16, val_loss=0.0475, train_loss_step=0.0342, train_loss_epoch=0.034]\n",
      "Epoch 29:  91%|█████████ | 10/11 [00:06<00:00,  1.52it/s, loss=0.0324, v_num=16, val_loss=0.0475, train_loss_step=0.0291, train_loss_epoch=0.034]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|██████████| 11/11 [00:06<00:00,  1.61it/s, loss=0.0324, v_num=16, val_loss=0.0456, train_loss_step=0.0329, train_loss_epoch=0.0307]\n",
      "Epoch 30:  91%|█████████ | 10/11 [00:06<00:00,  1.55it/s, loss=0.0296, v_num=16, val_loss=0.0456, train_loss_step=0.0275, train_loss_epoch=0.0307]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s, loss=0.0296, v_num=16, val_loss=0.0467, train_loss_step=0.0268, train_loss_epoch=0.0285]\n",
      "Epoch 31:  91%|█████████ | 10/11 [00:06<00:00,  1.52it/s, loss=0.0285, v_num=16, val_loss=0.0467, train_loss_step=0.0289, train_loss_epoch=0.0285]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|██████████| 11/11 [00:06<00:00,  1.63it/s, loss=0.0285, v_num=16, val_loss=0.0464, train_loss_step=0.0266, train_loss_epoch=0.0286]\n",
      "Epoch 32:  91%|█████████ | 10/11 [00:06<00:00,  1.55it/s, loss=0.0285, v_num=16, val_loss=0.0464, train_loss_step=0.0253, train_loss_epoch=0.0286]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|██████████| 11/11 [00:06<00:00,  1.66it/s, loss=0.0285, v_num=16, val_loss=0.0463, train_loss_step=0.0276, train_loss_epoch=0.0283]\n",
      "Epoch 33:  91%|█████████ | 10/11 [00:06<00:00,  1.54it/s, loss=0.0279, v_num=16, val_loss=0.0463, train_loss_step=0.0265, train_loss_epoch=0.0283]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s, loss=0.0279, v_num=16, val_loss=0.0461, train_loss_step=0.028, train_loss_epoch=0.0274] \n",
      "Epoch 33: 100%|██████████| 11/11 [00:06<00:00,  1.63it/s, loss=0.0279, v_num=16, val_loss=0.0461, train_loss_step=0.028, train_loss_epoch=0.0274]\n",
      "[15]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 618   \n",
      "3  | prescalers                         | ModuleDict                      | 272   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 25.3 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 23.2 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 66.6 K\n",
      "12 | lstm_decoder                       | LSTM                            | 66.6 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "331 K     Trainable params\n",
      "0         Non-trainable params\n",
      "331 K     Total params\n",
      "1.327     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]/home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  91%|█████████ | 10/11 [00:06<00:00,  1.56it/s, loss=0.161, v_num=17, val_loss=0.156, train_loss_step=0.148]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 11/11 [00:06<00:00,  1.67it/s, loss=0.161, v_num=17, val_loss=0.127, train_loss_step=0.145, train_loss_epoch=0.161]\n",
      "Epoch 1:  91%|█████████ | 10/11 [00:06<00:00,  1.49it/s, loss=0.149, v_num=17, val_loss=0.127, train_loss_step=0.136, train_loss_epoch=0.161]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 11/11 [00:06<00:00,  1.59it/s, loss=0.149, v_num=17, val_loss=0.125, train_loss_step=0.136, train_loss_epoch=0.137]\n",
      "Epoch 2:  91%|█████████ | 10/11 [00:06<00:00,  1.56it/s, loss=0.132, v_num=17, val_loss=0.125, train_loss_step=0.125, train_loss_epoch=0.137]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 11/11 [00:06<00:00,  1.67it/s, loss=0.132, v_num=17, val_loss=0.143, train_loss_step=0.123, train_loss_epoch=0.127]\n",
      "Epoch 3:  91%|█████████ | 10/11 [00:06<00:00,  1.54it/s, loss=0.123, v_num=17, val_loss=0.143, train_loss_step=0.115, train_loss_epoch=0.127]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 11/11 [00:06<00:00,  1.64it/s, loss=0.123, v_num=17, val_loss=0.128, train_loss_step=0.116, train_loss_epoch=0.120]\n",
      "Epoch 4:  91%|█████████ | 10/11 [00:06<00:00,  1.54it/s, loss=0.115, v_num=17, val_loss=0.128, train_loss_step=0.108, train_loss_epoch=0.120]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s, loss=0.115, v_num=17, val_loss=0.131, train_loss_step=0.106, train_loss_epoch=0.111]\n",
      "Epoch 5:  91%|█████████ | 10/11 [00:06<00:00,  1.54it/s, loss=0.106, v_num=17, val_loss=0.131, train_loss_step=0.0987, train_loss_epoch=0.111]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 11/11 [00:06<00:00,  1.64it/s, loss=0.106, v_num=17, val_loss=0.122, train_loss_step=0.0975, train_loss_epoch=0.102]\n",
      "Epoch 6:  91%|█████████ | 10/11 [00:06<00:00,  1.52it/s, loss=0.0984, v_num=17, val_loss=0.122, train_loss_step=0.0938, train_loss_epoch=0.102]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 11/11 [00:06<00:00,  1.62it/s, loss=0.0984, v_num=17, val_loss=0.115, train_loss_step=0.0889, train_loss_epoch=0.0945]\n",
      "Epoch 7:  91%|█████████ | 10/11 [00:06<00:00,  1.51it/s, loss=0.0914, v_num=17, val_loss=0.115, train_loss_step=0.0849, train_loss_epoch=0.0945]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 11/11 [00:06<00:00,  1.61it/s, loss=0.0914, v_num=17, val_loss=0.112, train_loss_step=0.0887, train_loss_epoch=0.0882]\n",
      "Epoch 8:  91%|█████████ | 10/11 [00:06<00:00,  1.52it/s, loss=0.0863, v_num=17, val_loss=0.112, train_loss_step=0.0831, train_loss_epoch=0.0882]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 11/11 [00:06<00:00,  1.62it/s, loss=0.0863, v_num=17, val_loss=0.103, train_loss_step=0.0816, train_loss_epoch=0.0845]\n",
      "Epoch 9:  91%|█████████ | 10/11 [00:06<00:00,  1.57it/s, loss=0.0823, v_num=17, val_loss=0.103, train_loss_step=0.0794, train_loss_epoch=0.0845]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 11/11 [00:06<00:00,  1.67it/s, loss=0.0823, v_num=17, val_loss=0.112, train_loss_step=0.0783, train_loss_epoch=0.0802]\n",
      "Epoch 10:  91%|█████████ | 10/11 [00:06<00:00,  1.53it/s, loss=0.0779, v_num=17, val_loss=0.112, train_loss_step=0.0746, train_loss_epoch=0.0802]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 11/11 [00:06<00:00,  1.64it/s, loss=0.0779, v_num=17, val_loss=0.0921, train_loss_step=0.0759, train_loss_epoch=0.0756]\n",
      "Epoch 11:  91%|█████████ | 10/11 [00:06<00:00,  1.55it/s, loss=0.0742, v_num=17, val_loss=0.0921, train_loss_step=0.0751, train_loss_epoch=0.0756]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 11/11 [00:06<00:00,  1.66it/s, loss=0.0742, v_num=17, val_loss=0.0969, train_loss_step=0.0699, train_loss_epoch=0.0729]\n",
      "Epoch 12:  91%|█████████ | 10/11 [00:06<00:00,  1.54it/s, loss=0.0712, v_num=17, val_loss=0.0969, train_loss_step=0.0705, train_loss_epoch=0.0729]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 11/11 [00:06<00:00,  1.66it/s, loss=0.0712, v_num=17, val_loss=0.0883, train_loss_step=0.0693, train_loss_epoch=0.0695]\n",
      "Epoch 13:  91%|█████████ | 10/11 [00:06<00:00,  1.54it/s, loss=0.0675, v_num=17, val_loss=0.0883, train_loss_step=0.0626, train_loss_epoch=0.0695]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s, loss=0.0675, v_num=17, val_loss=0.0991, train_loss_step=0.0617, train_loss_epoch=0.0656]\n",
      "Epoch 14:  91%|█████████ | 10/11 [00:06<00:00,  1.56it/s, loss=0.0633, v_num=17, val_loss=0.0991, train_loss_step=0.0586, train_loss_epoch=0.0656]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 11/11 [00:06<00:00,  1.67it/s, loss=0.0633, v_num=17, val_loss=0.0979, train_loss_step=0.0586, train_loss_epoch=0.0611]\n",
      "Epoch 15:  91%|█████████ | 10/11 [00:06<00:00,  1.54it/s, loss=0.0596, v_num=17, val_loss=0.0979, train_loss_step=0.058, train_loss_epoch=0.0611] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s, loss=0.0596, v_num=17, val_loss=0.0875, train_loss_step=0.0581, train_loss_epoch=0.058]\n",
      "Epoch 16:  91%|█████████ | 10/11 [00:06<00:00,  1.55it/s, loss=0.0567, v_num=17, val_loss=0.0875, train_loss_step=0.0527, train_loss_epoch=0.058]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 11/11 [00:06<00:00,  1.66it/s, loss=0.0567, v_num=17, val_loss=0.0927, train_loss_step=0.0547, train_loss_epoch=0.0555]\n",
      "Epoch 17:  91%|█████████ | 10/11 [00:06<00:00,  1.53it/s, loss=0.0538, v_num=17, val_loss=0.0927, train_loss_step=0.0532, train_loss_epoch=0.0555]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 11/11 [00:06<00:00,  1.64it/s, loss=0.0538, v_num=17, val_loss=0.101, train_loss_step=0.0509, train_loss_epoch=0.0522] \n",
      "Epoch 18:  91%|█████████ | 10/11 [00:06<00:00,  1.53it/s, loss=0.0509, v_num=17, val_loss=0.101, train_loss_step=0.0497, train_loss_epoch=0.0522]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 11/11 [00:06<00:00,  1.64it/s, loss=0.0509, v_num=17, val_loss=0.0977, train_loss_step=0.0489, train_loss_epoch=0.0495]\n",
      "Epoch 19:  91%|█████████ | 10/11 [00:06<00:00,  1.56it/s, loss=0.0485, v_num=17, val_loss=0.0977, train_loss_step=0.0461, train_loss_epoch=0.0495]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 11/11 [00:06<00:00,  1.66it/s, loss=0.0485, v_num=17, val_loss=0.0985, train_loss_step=0.0455, train_loss_epoch=0.0476]\n",
      "Epoch 20:  91%|█████████ | 10/11 [00:06<00:00,  1.54it/s, loss=0.0459, v_num=17, val_loss=0.0985, train_loss_step=0.0437, train_loss_epoch=0.0476]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|██████████| 11/11 [00:06<00:00,  1.64it/s, loss=0.0459, v_num=17, val_loss=0.103, train_loss_step=0.0422, train_loss_epoch=0.0442] \n",
      "Epoch 21:  91%|█████████ | 10/11 [00:06<00:00,  1.53it/s, loss=0.043, v_num=17, val_loss=0.103, train_loss_step=0.0417, train_loss_epoch=0.0442] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|██████████| 11/11 [00:06<00:00,  1.63it/s, loss=0.043, v_num=17, val_loss=0.0989, train_loss_step=0.0424, train_loss_epoch=0.0419]\n",
      "Epoch 22:  91%|█████████ | 10/11 [00:06<00:00,  1.48it/s, loss=0.0414, v_num=17, val_loss=0.0989, train_loss_step=0.0416, train_loss_epoch=0.0419]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|██████████| 11/11 [00:06<00:00,  1.59it/s, loss=0.0414, v_num=17, val_loss=0.0986, train_loss_step=0.0406, train_loss_epoch=0.0408]\n",
      "Epoch 23:  91%|█████████ | 10/11 [00:06<00:00,  1.51it/s, loss=0.0405, v_num=17, val_loss=0.0986, train_loss_step=0.0404, train_loss_epoch=0.0408]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|██████████| 11/11 [00:06<00:00,  1.62it/s, loss=0.0405, v_num=17, val_loss=0.099, train_loss_step=0.0397, train_loss_epoch=0.0402] \n",
      "Epoch 24:  91%|█████████ | 10/11 [00:06<00:00,  1.53it/s, loss=0.04, v_num=17, val_loss=0.099, train_loss_step=0.0398, train_loss_epoch=0.0402]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|██████████| 11/11 [00:06<00:00,  1.63it/s, loss=0.04, v_num=17, val_loss=0.0985, train_loss_step=0.0411, train_loss_epoch=0.0399]\n",
      "Epoch 25:  91%|█████████ | 10/11 [00:06<00:00,  1.53it/s, loss=0.0395, v_num=17, val_loss=0.0985, train_loss_step=0.0407, train_loss_epoch=0.0399]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|██████████| 11/11 [00:06<00:00,  1.64it/s, loss=0.0395, v_num=17, val_loss=0.0987, train_loss_step=0.0383, train_loss_epoch=0.0392]\n",
      "Epoch 25: 100%|██████████| 11/11 [00:06<00:00,  1.63it/s, loss=0.0395, v_num=17, val_loss=0.0987, train_loss_step=0.0383, train_loss_epoch=0.0392]\n",
      "[38, 58, 43]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 626   \n",
      "3  | prescalers                         | ModuleDict                      | 272   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.1 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 25.3 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 23.2 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 66.6 K\n",
      "12 | lstm_decoder                       | LSTM                            | 66.6 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "331 K     Trainable params\n",
      "0         Non-trainable params\n",
      "331 K     Total params\n",
      "1.327     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]/home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  91%|█████████ | 10/11 [00:06<00:00,  1.57it/s, loss=0.25, v_num=18, val_loss=0.267, train_loss_step=0.227] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 11/11 [00:06<00:00,  1.68it/s, loss=0.25, v_num=18, val_loss=0.178, train_loss_step=0.189, train_loss_epoch=0.250]\n",
      "Epoch 1:  91%|█████████ | 10/11 [00:06<00:00,  1.56it/s, loss=0.203, v_num=18, val_loss=0.178, train_loss_step=0.135, train_loss_epoch=0.250]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 11/11 [00:06<00:00,  1.68it/s, loss=0.203, v_num=18, val_loss=0.122, train_loss_step=0.125, train_loss_epoch=0.155]\n",
      "Epoch 2:  91%|█████████ | 10/11 [00:06<00:00,  1.55it/s, loss=0.138, v_num=18, val_loss=0.122, train_loss_step=0.113, train_loss_epoch=0.155]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 11/11 [00:06<00:00,  1.65it/s, loss=0.138, v_num=18, val_loss=0.117, train_loss_step=0.118, train_loss_epoch=0.120]\n",
      "Epoch 3:  91%|█████████ | 10/11 [00:06<00:00,  1.54it/s, loss=0.117, v_num=18, val_loss=0.117, train_loss_step=0.105, train_loss_epoch=0.120]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 11/11 [00:06<00:00,  1.64it/s, loss=0.117, v_num=18, val_loss=0.103, train_loss_step=0.102, train_loss_epoch=0.114]\n",
      "Epoch 4:  91%|█████████ | 10/11 [00:06<00:00,  1.53it/s, loss=0.105, v_num=18, val_loss=0.103, train_loss_step=0.0873, train_loss_epoch=0.114]"
     ]
    }
   ],
   "source": [
    "for idx in range(5, 28):    \n",
    "\n",
    "    print(group[idx])\n",
    "\n",
    "    data = data_bag[idx]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    max_prediction_length = 168\n",
    "    max_encoder_length = 336\n",
    "    training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= training_cutoff],\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"kWH\",\n",
    "        group_ids=[\"num\"],\n",
    "        min_encoder_length=max_encoder_length,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=max_prediction_length,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[\"num\", \"energy_group\"],\n",
    "        static_reals=[\"non_electric_aircondition\", \"sunlight\"],\n",
    "        time_varying_known_categoricals=[\"day_of_Week\", \"day_of_month\", \"24Hour\", \"holiday\", \"Weekend\", \"hour_cat\"],\n",
    "        time_varying_known_reals=[\"C\", \"m/s\", \"wet\", \"mm\", \"hr\", \"time_idx\", \"day\", \"Week\", \"perceived_temperature\", \"discomfort_index\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[\"kWH\"],\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "    batch_size = 64  # set this between 32 to 128\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=8)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=1)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=500,\n",
    "        gpus=1,\n",
    "        weights_summary=\"top\",\n",
    "        gradient_clip_val=0.1,\n",
    "        limit_train_batches=10, \n",
    "        callbacks=[lr_logger, early_stop_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.03,\n",
    "        hidden_size=64,\n",
    "        lstm_layers = 2,\n",
    "        attention_head_size=4,\n",
    "        dropout=0.15,\n",
    "        hidden_continuous_size=8,\n",
    "        output_size=1,\n",
    "        loss=SMAPE(),\n",
    "        log_interval=0,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    # fit network\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd02625b230b0936c03a7a859dabe4a10a429313c958b60e79e367b6eadd1f82f0b",
   "display_name": "Python 3.9.1 64-bit ('torch1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}