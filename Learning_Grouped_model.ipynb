{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss, MultiHorizonMetric\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Refined_Data/Grouped_Data/Input_Data.csv\"\n",
    "\n",
    "group = [\n",
    "    [4],[11, 12],[34],[40],[10],[42],[41]                                   # 0~ 6\n",
    "    ,[6, 8, 13, 17, 26, 48, 53, 55, 56],[7, 18],[27, 57],[35, 46, 47],[25]  # 7 ~ 11\n",
    "    ,[1, 31],[9, 32],[3],[33]                                               # 12 ~ 15\n",
    "    ,[2, 14, 22, 37, 44, 52, 54],[15],[38, 58, 43],[29, 39],[45],[23]       # 16 ~ 21\n",
    "    ,[5],[16, 24],[19, 20, 21, 49, 50, 51],[28, 36, 60],[59], [30]          # 22 ~ 26\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, parse_dates = [\"date_time\"])\n",
    "\n",
    "data['num']     =   data['num'].apply(str)\n",
    "data['Week']    =   data['Week'].apply(str)\n",
    "data['24Hour']  =   data['24Hour'].apply(str)\n",
    "data['holiday'] =   data['holiday'].apply(str)\n",
    "data['Weekend'] =   data['Weekend'].apply(str)\n",
    "data['energy_group'] = data['energy_group'].apply(str)\n",
    "data['hour_cat']=   data['hour_cat'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bag = [data.loc[data[\"energy_group\"] == str(i)].copy() for i in range(len(group))]\r\n",
    "\r\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 257   \n",
      "3  | prescalers                         | ModuleDict                      | 240   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.0 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 18.1 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "254 K     Trainable params\n",
      "0         Non-trainable params\n",
      "254 K     Total params\n",
      "1.017     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]/home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  91%|█████████ | 10/11 [00:05<00:00,  1.82it/s, loss=0.624, v_num=0, val_loss=0.703, train_loss_step=0.596]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 11/11 [00:05<00:00,  1.95it/s, loss=0.624, v_num=0, val_loss=0.494, train_loss_step=0.572, train_loss_epoch=0.624]\n",
      "Epoch 1:  91%|█████████ | 10/11 [00:05<00:00,  1.85it/s, loss=0.574, v_num=0, val_loss=0.494, train_loss_step=0.508, train_loss_epoch=0.624]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 11/11 [00:05<00:00,  1.97it/s, loss=0.574, v_num=0, val_loss=0.448, train_loss_step=0.491, train_loss_epoch=0.523]\n",
      "Epoch 2:  91%|█████████ | 10/11 [00:05<00:00,  1.85it/s, loss=0.483, v_num=0, val_loss=0.448, train_loss_step=0.419, train_loss_epoch=0.523]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 11/11 [00:05<00:00,  1.98it/s, loss=0.483, v_num=0, val_loss=0.437, train_loss_step=0.404, train_loss_epoch=0.442]\n",
      "Epoch 3:  91%|█████████ | 10/11 [00:05<00:00,  1.75it/s, loss=0.423, v_num=0, val_loss=0.437, train_loss_step=0.420, train_loss_epoch=0.442]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 11/11 [00:05<00:00,  1.88it/s, loss=0.423, v_num=0, val_loss=0.406, train_loss_step=0.393, train_loss_epoch=0.404]\n",
      "Epoch 4:  91%|█████████ | 10/11 [00:05<00:00,  1.83it/s, loss=0.389, v_num=0, val_loss=0.406, train_loss_step=0.364, train_loss_epoch=0.404]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 11/11 [00:05<00:00,  1.94it/s, loss=0.389, v_num=0, val_loss=0.372, train_loss_step=0.374, train_loss_epoch=0.375]\n",
      "Epoch 5:  91%|█████████ | 10/11 [00:05<00:00,  1.85it/s, loss=0.362, v_num=0, val_loss=0.372, train_loss_step=0.330, train_loss_epoch=0.375]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 11/11 [00:05<00:00,  1.97it/s, loss=0.362, v_num=0, val_loss=0.353, train_loss_step=0.344, train_loss_epoch=0.349]\n",
      "Epoch 6:  91%|█████████ | 10/11 [00:05<00:00,  1.77it/s, loss=0.335, v_num=0, val_loss=0.353, train_loss_step=0.300, train_loss_epoch=0.349]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 11/11 [00:05<00:00,  1.86it/s, loss=0.335, v_num=0, val_loss=0.279, train_loss_step=0.301, train_loss_epoch=0.321]\n",
      "Epoch 7:  91%|█████████ | 10/11 [00:05<00:00,  1.79it/s, loss=0.303, v_num=0, val_loss=0.279, train_loss_step=0.272, train_loss_epoch=0.321]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 11/11 [00:05<00:00,  1.90it/s, loss=0.303, v_num=0, val_loss=0.277, train_loss_step=0.275, train_loss_epoch=0.284]\n",
      "Epoch 8:  91%|█████████ | 10/11 [00:05<00:00,  1.87it/s, loss=0.267, v_num=0, val_loss=0.277, train_loss_step=0.233, train_loss_epoch=0.284]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 11/11 [00:05<00:00,  1.99it/s, loss=0.267, v_num=0, val_loss=0.239, train_loss_step=0.234, train_loss_epoch=0.250]\n",
      "Epoch 9:  91%|█████████ | 10/11 [00:05<00:00,  1.84it/s, loss=0.234, v_num=0, val_loss=0.239, train_loss_step=0.201, train_loss_epoch=0.250]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 11/11 [00:05<00:00,  1.96it/s, loss=0.234, v_num=0, val_loss=0.223, train_loss_step=0.203, train_loss_epoch=0.217]\n",
      "Epoch 10:  91%|█████████ | 10/11 [00:05<00:00,  1.86it/s, loss=0.205, v_num=0, val_loss=0.223, train_loss_step=0.193, train_loss_epoch=0.217]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 11/11 [00:05<00:00,  1.99it/s, loss=0.205, v_num=0, val_loss=0.227, train_loss_step=0.191, train_loss_epoch=0.194]\n",
      "Epoch 11:  91%|█████████ | 10/11 [00:05<00:00,  1.84it/s, loss=0.185, v_num=0, val_loss=0.227, train_loss_step=0.168, train_loss_epoch=0.194]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 11/11 [00:05<00:00,  1.95it/s, loss=0.185, v_num=0, val_loss=0.215, train_loss_step=0.170, train_loss_epoch=0.177]\n",
      "Epoch 12:  91%|█████████ | 10/11 [00:05<00:00,  1.86it/s, loss=0.177, v_num=0, val_loss=0.215, train_loss_step=0.176, train_loss_epoch=0.177]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 11/11 [00:05<00:00,  1.97it/s, loss=0.177, v_num=0, val_loss=0.213, train_loss_step=0.165, train_loss_epoch=0.177]\n",
      "Epoch 13:  91%|█████████ | 10/11 [00:05<00:00,  1.87it/s, loss=0.174, v_num=0, val_loss=0.213, train_loss_step=0.173, train_loss_epoch=0.177]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 11/11 [00:05<00:00,  1.98it/s, loss=0.174, v_num=0, val_loss=0.242, train_loss_step=0.170, train_loss_epoch=0.171]\n",
      "Epoch 14:  91%|█████████ | 10/11 [00:05<00:00,  1.75it/s, loss=0.172, v_num=0, val_loss=0.242, train_loss_step=0.170, train_loss_epoch=0.171]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 11/11 [00:05<00:00,  1.86it/s, loss=0.172, v_num=0, val_loss=0.221, train_loss_step=0.176, train_loss_epoch=0.173]\n",
      "Epoch 15:  91%|█████████ | 10/11 [00:05<00:00,  1.83it/s, loss=0.172, v_num=0, val_loss=0.221, train_loss_step=0.189, train_loss_epoch=0.173]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 11/11 [00:05<00:00,  1.95it/s, loss=0.172, v_num=0, val_loss=0.247, train_loss_step=0.170, train_loss_epoch=0.171]\n",
      "Epoch 16:  91%|█████████ | 10/11 [00:05<00:00,  1.84it/s, loss=0.171, v_num=0, val_loss=0.247, train_loss_step=0.160, train_loss_epoch=0.171]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 11/11 [00:05<00:00,  1.96it/s, loss=0.171, v_num=0, val_loss=0.239, train_loss_step=0.161, train_loss_epoch=0.170]\n",
      "Epoch 17:  91%|█████████ | 10/11 [00:05<00:00,  1.86it/s, loss=0.167, v_num=0, val_loss=0.239, train_loss_step=0.168, train_loss_epoch=0.170]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 11/11 [00:05<00:00,  1.96it/s, loss=0.167, v_num=0, val_loss=0.229, train_loss_step=0.160, train_loss_epoch=0.164]\n",
      "Epoch 18:  91%|█████████ | 10/11 [00:05<00:00,  1.85it/s, loss=0.163, v_num=0, val_loss=0.229, train_loss_step=0.168, train_loss_epoch=0.164]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 11/11 [00:05<00:00,  1.96it/s, loss=0.163, v_num=0, val_loss=0.251, train_loss_step=0.165, train_loss_epoch=0.163]\n",
      "Epoch 19:  91%|█████████ | 10/11 [00:05<00:00,  1.81it/s, loss=0.162, v_num=0, val_loss=0.251, train_loss_step=0.166, train_loss_epoch=0.163]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 11/11 [00:05<00:00,  1.93it/s, loss=0.162, v_num=0, val_loss=0.240, train_loss_step=0.157, train_loss_epoch=0.161]\n",
      "Epoch 20:  91%|█████████ | 10/11 [00:05<00:00,  1.82it/s, loss=0.16, v_num=0, val_loss=0.240, train_loss_step=0.159, train_loss_epoch=0.161]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|██████████| 11/11 [00:05<00:00,  1.94it/s, loss=0.16, v_num=0, val_loss=0.244, train_loss_step=0.156, train_loss_epoch=0.160]\n",
      "Epoch 21:  91%|█████████ | 10/11 [00:05<00:00,  1.80it/s, loss=0.16, v_num=0, val_loss=0.244, train_loss_step=0.160, train_loss_epoch=0.160] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|██████████| 11/11 [00:05<00:00,  1.90it/s, loss=0.16, v_num=0, val_loss=0.247, train_loss_step=0.169, train_loss_epoch=0.161]\n",
      "Epoch 22:  55%|█████▍    | 6/11 [00:03<00:02,  1.72it/s, loss=0.161, v_num=0, val_loss=0.247, train_loss_step=0.163, train_loss_epoch=0.161]"
     ]
    }
   ],
   "source": [
    "for idx in range(len(data_bag)):\n",
    "\n",
    "    data = data_bag[idx]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    max_prediction_length = 168\n",
    "    max_encoder_length = 336\n",
    "    training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= training_cutoff],\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"kWH\",\n",
    "        group_ids=[\"num\"],\n",
    "        min_encoder_length=max_encoder_length,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=max_prediction_length,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[\"num\", \"energy_group\"],\n",
    "        static_reals=[\"non_electric_aircondition\", \"sunlight\"],\n",
    "        time_varying_known_categoricals=[\"Week\", \"24Hour\", \"holiday\", \"Weekend\", \"hour_cat\"],\n",
    "        time_varying_known_reals=[\"C\", \"m/s\", \"wet\", \"mm\", \"hr\", \"time_idx\", \"perceived_temperature\", \"discomfort_index\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[\"kWH\"],\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "    batch_size = 64  # set this between 32 to 128\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=8)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=1)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=500,\n",
    "        gpus=1,\n",
    "        weights_summary=\"top\",\n",
    "        gradient_clip_val=0.1,\n",
    "        limit_train_batches=10, \n",
    "        callbacks=[lr_logger, early_stop_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.01,\n",
    "        hidden_size=64,\n",
    "        lstm_layers = 1,\n",
    "        attention_head_size=4,\n",
    "        dropout=0.15,\n",
    "        hidden_continuous_size=8,\n",
    "        output_size=1,\n",
    "        loss=SMAPE(),\n",
    "        log_interval=0,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    # fit network\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd02625b230b0936c03a7a859dabe4a10a429313c958b60e79e367b6eadd1f82f0b",
   "display_name": "Python 3.9.1 64-bit ('torch1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}