{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Refined_Data/Grouped_Data/Input_Data2.csv\"\n",
    "\n",
    "building_num = [[4]]\n",
    "\n",
    "repeat = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, parse_dates = [\"date_time\"])\n",
    "\n",
    "data['num']     =   data['num'].apply(str)\n",
    "data['Week']    =   data['Week'].apply(str)\n",
    "data['day_of_Week']     =   data['day_of_Week'].apply(str)\n",
    "data['day_of_month']    =   data['day_of_month'].apply(str)\n",
    "data['day']     =   data['day'].apply(str)\n",
    "data['24Hour']  =   data['24Hour'].apply(str)\n",
    "data['holiday'] =   data['holiday'].apply(str)\n",
    "data['Weekend'] =   data['Weekend'].apply(str)\n",
    "data['energy_group'] = data['energy_group'].apply(str)\n",
    "data['hour_cat']=   data['hour_cat'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bag = [data.loc[data[\"num\"] == str(building_num[i])].copy() for i in range(len(building_num))]\n",
    "\n",
    "data_bag = []\n",
    "for buildings in building_num:\n",
    "    temp = data.loc[data[\"num\"] == str(buildings[0])].copy()\n",
    "    for num_idx in range(1, len(buildings)):\n",
    "        temp = pd.concat([temp, data.loc[data[\"num\"] == str(buildings[num_idx])].copy()])\n",
    "    data_bag.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "_loss_step=0.169, train_loss_epoch=0.162]\n",
      "Epoch 9:  95%|█████████▌| 21/22 [00:12<00:00,  1.73it/s, loss=0.158, v_num=3, val_loss=0.217, train_loss_step=0.153, train_loss_epoch=0.162]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 22/22 [00:12<00:00,  1.80it/s, loss=0.158, v_num=3, val_loss=0.205, train_loss_step=0.161, train_loss_epoch=0.158]\n",
      "Epoch 10:  95%|█████████▌| 21/22 [00:11<00:00,  1.77it/s, loss=0.156, v_num=3, val_loss=0.205, train_loss_step=0.150, train_loss_epoch=0.158]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 22/22 [00:11<00:00,  1.84it/s, loss=0.156, v_num=3, val_loss=0.224, train_loss_step=0.167, train_loss_epoch=0.156]\n",
      "Epoch 11:  95%|█████████▌| 21/22 [00:12<00:00,  1.72it/s, loss=0.148, v_num=3, val_loss=0.224, train_loss_step=0.142, train_loss_epoch=0.156]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 22/22 [00:12<00:00,  1.79it/s, loss=0.148, v_num=3, val_loss=0.212, train_loss_step=0.131, train_loss_epoch=0.148]\n",
      "Epoch 12:  95%|█████████▌| 21/22 [00:12<00:00,  1.73it/s, loss=0.145, v_num=3, val_loss=0.212, train_loss_step=0.134, train_loss_epoch=0.148]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 22/22 [00:12<00:00,  1.80it/s, loss=0.145, v_num=3, val_loss=0.207, train_loss_step=0.148, train_loss_epoch=0.145]\n",
      "Epoch 13:  95%|█████████▌| 21/22 [00:11<00:00,  1.77it/s, loss=0.143, v_num=3, val_loss=0.207, train_loss_step=0.142, train_loss_epoch=0.145]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 22/22 [00:11<00:00,  1.84it/s, loss=0.143, v_num=3, val_loss=0.207, train_loss_step=0.144, train_loss_epoch=0.143]\n",
      "Epoch 14:  95%|█████████▌| 21/22 [00:12<00:00,  1.72it/s, loss=0.141, v_num=3, val_loss=0.207, train_loss_step=0.142, train_loss_epoch=0.143]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 22/22 [00:12<00:00,  1.79it/s, loss=0.141, v_num=3, val_loss=0.211, train_loss_step=0.130, train_loss_epoch=0.141]\n",
      "Epoch 15:  95%|█████████▌| 21/22 [00:12<00:00,  1.75it/s, loss=0.139, v_num=3, val_loss=0.211, train_loss_step=0.140, train_loss_epoch=0.141]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 22/22 [00:12<00:00,  1.82it/s, loss=0.139, v_num=3, val_loss=0.216, train_loss_step=0.146, train_loss_epoch=0.139]\n",
      "Epoch 15: 100%|██████████| 22/22 [00:12<00:00,  1.82it/s, loss=0.139, v_num=3, val_loss=0.216, train_loss_step=0.146, train_loss_epoch=0.139]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 277   \n",
      "3  | prescalers                         | ModuleDict                      | 240   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.6 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 18.5 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "255 K     Trainable params\n",
      "0         Non-trainable params\n",
      "255 K     Total params\n",
      "1.021     Total estimated model params size (MB)\n",
      "Epoch 0:   0%|          | 0/22 [00:00<?, ?it/s] /home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  95%|█████████▌| 21/22 [00:11<00:00,  1.75it/s, loss=0.516, v_num=4, val_loss=0.598, train_loss_step=0.459]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 22/22 [00:12<00:00,  1.83it/s, loss=0.516, v_num=4, val_loss=0.389, train_loss_step=0.446, train_loss_epoch=0.519]\n",
      "Epoch 1:  95%|█████████▌| 21/22 [00:12<00:00,  1.68it/s, loss=0.384, v_num=4, val_loss=0.389, train_loss_step=0.339, train_loss_epoch=0.519]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 22/22 [00:12<00:00,  1.75it/s, loss=0.384, v_num=4, val_loss=0.373, train_loss_step=0.348, train_loss_epoch=0.387]\n",
      "Epoch 2:  95%|█████████▌| 21/22 [00:12<00:00,  1.72it/s, loss=0.32, v_num=4, val_loss=0.373, train_loss_step=0.286, train_loss_epoch=0.387] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 22/22 [00:12<00:00,  1.79it/s, loss=0.32, v_num=4, val_loss=0.337, train_loss_step=0.299, train_loss_epoch=0.321]\n",
      "Epoch 3:  95%|█████████▌| 21/22 [00:12<00:00,  1.72it/s, loss=0.269, v_num=4, val_loss=0.337, train_loss_step=0.254, train_loss_epoch=0.321]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 22/22 [00:12<00:00,  1.79it/s, loss=0.269, v_num=4, val_loss=0.273, train_loss_step=0.238, train_loss_epoch=0.273]\n",
      "Epoch 4:  95%|█████████▌| 21/22 [00:12<00:00,  1.67it/s, loss=0.206, v_num=4, val_loss=0.273, train_loss_step=0.197, train_loss_epoch=0.273]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 22/22 [00:12<00:00,  1.74it/s, loss=0.206, v_num=4, val_loss=0.234, train_loss_step=0.191, train_loss_epoch=0.207]\n",
      "Epoch 5:  95%|█████████▌| 21/22 [00:12<00:00,  1.68it/s, loss=0.185, v_num=4, val_loss=0.234, train_loss_step=0.186, train_loss_epoch=0.207]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 22/22 [00:12<00:00,  1.76it/s, loss=0.185, v_num=4, val_loss=0.257, train_loss_step=0.179, train_loss_epoch=0.186]\n",
      "Epoch 6:  95%|█████████▌| 21/22 [00:12<00:00,  1.73it/s, loss=0.174, v_num=4, val_loss=0.257, train_loss_step=0.170, train_loss_epoch=0.186]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 22/22 [00:12<00:00,  1.81it/s, loss=0.174, v_num=4, val_loss=0.252, train_loss_step=0.172, train_loss_epoch=0.174]\n",
      "Epoch 7:  95%|█████████▌| 21/22 [00:12<00:00,  1.69it/s, loss=0.172, v_num=4, val_loss=0.252, train_loss_step=0.159, train_loss_epoch=0.174]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 22/22 [00:12<00:00,  1.76it/s, loss=0.172, v_num=4, val_loss=0.247, train_loss_step=0.169, train_loss_epoch=0.172]\n",
      "Epoch 8:  95%|█████████▌| 21/22 [00:12<00:00,  1.69it/s, loss=0.154, v_num=4, val_loss=0.247, train_loss_step=0.152, train_loss_epoch=0.172]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 22/22 [00:12<00:00,  1.77it/s, loss=0.154, v_num=4, val_loss=0.229, train_loss_step=0.155, train_loss_epoch=0.155]\n",
      "Epoch 9:  95%|█████████▌| 21/22 [00:11<00:00,  1.76it/s, loss=0.151, v_num=4, val_loss=0.229, train_loss_step=0.165, train_loss_epoch=0.155]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 22/22 [00:12<00:00,  1.83it/s, loss=0.151, v_num=4, val_loss=0.244, train_loss_step=0.158, train_loss_epoch=0.151]\n",
      "Epoch 10:  95%|█████████▌| 21/22 [00:11<00:00,  1.80it/s, loss=0.142, v_num=4, val_loss=0.244, train_loss_step=0.130, train_loss_epoch=0.151]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 22/22 [00:11<00:00,  1.87it/s, loss=0.142, v_num=4, val_loss=0.225, train_loss_step=0.131, train_loss_epoch=0.142]\n",
      "Epoch 11:  95%|█████████▌| 21/22 [00:12<00:00,  1.71it/s, loss=0.133, v_num=4, val_loss=0.225, train_loss_step=0.132, train_loss_epoch=0.142]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 22/22 [00:12<00:00,  1.79it/s, loss=0.133, v_num=4, val_loss=0.232, train_loss_step=0.141, train_loss_epoch=0.133]\n",
      "Epoch 12:  95%|█████████▌| 21/22 [00:12<00:00,  1.74it/s, loss=0.122, v_num=4, val_loss=0.232, train_loss_step=0.123, train_loss_epoch=0.133]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 22/22 [00:12<00:00,  1.82it/s, loss=0.122, v_num=4, val_loss=0.259, train_loss_step=0.128, train_loss_epoch=0.122]\n",
      "Epoch 13:  95%|█████████▌| 21/22 [00:12<00:00,  1.71it/s, loss=0.122, v_num=4, val_loss=0.259, train_loss_step=0.128, train_loss_epoch=0.122]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 22/22 [00:12<00:00,  1.78it/s, loss=0.122, v_num=4, val_loss=0.256, train_loss_step=0.118, train_loss_epoch=0.123]\n",
      "Epoch 14:  95%|█████████▌| 21/22 [00:12<00:00,  1.73it/s, loss=0.114, v_num=4, val_loss=0.256, train_loss_step=0.116, train_loss_epoch=0.123]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 22/22 [00:12<00:00,  1.81it/s, loss=0.114, v_num=4, val_loss=0.217, train_loss_step=0.104, train_loss_epoch=0.114]\n",
      "Epoch 15:  95%|█████████▌| 21/22 [00:11<00:00,  1.78it/s, loss=0.12, v_num=4, val_loss=0.217, train_loss_step=0.145, train_loss_epoch=0.114] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 22/22 [00:11<00:00,  1.85it/s, loss=0.12, v_num=4, val_loss=0.227, train_loss_step=0.137, train_loss_epoch=0.120]\n",
      "Epoch 16:  95%|█████████▌| 21/22 [00:11<00:00,  1.80it/s, loss=0.111, v_num=4, val_loss=0.227, train_loss_step=0.102, train_loss_epoch=0.120]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 22/22 [00:11<00:00,  1.87it/s, loss=0.111, v_num=4, val_loss=0.229, train_loss_step=0.111, train_loss_epoch=0.110]\n",
      "Epoch 17:  95%|█████████▌| 21/22 [00:11<00:00,  1.79it/s, loss=0.114, v_num=4, val_loss=0.229, train_loss_step=0.139, train_loss_epoch=0.110]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 22/22 [00:11<00:00,  1.87it/s, loss=0.114, v_num=4, val_loss=0.251, train_loss_step=0.147, train_loss_epoch=0.114]\n",
      "Epoch 18:  95%|█████████▌| 21/22 [00:11<00:00,  1.78it/s, loss=0.104, v_num=4, val_loss=0.251, train_loss_step=0.108, train_loss_epoch=0.114] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 22/22 [00:11<00:00,  1.85it/s, loss=0.104, v_num=4, val_loss=0.265, train_loss_step=0.117, train_loss_epoch=0.104]\n",
      "Epoch 19:  95%|█████████▌| 21/22 [00:11<00:00,  1.75it/s, loss=0.1, v_num=4, val_loss=0.265, train_loss_step=0.0971, train_loss_epoch=0.104]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 22/22 [00:12<00:00,  1.83it/s, loss=0.1, v_num=4, val_loss=0.226, train_loss_step=0.0958, train_loss_epoch=0.101]\n",
      "Epoch 20:  95%|█████████▌| 21/22 [00:12<00:00,  1.75it/s, loss=0.0851, v_num=4, val_loss=0.226, train_loss_step=0.0838, train_loss_epoch=0.101]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|██████████| 22/22 [00:12<00:00,  1.82it/s, loss=0.0851, v_num=4, val_loss=0.232, train_loss_step=0.0841, train_loss_epoch=0.0852]\n",
      "Epoch 21:  95%|█████████▌| 21/22 [00:11<00:00,  1.77it/s, loss=0.0823, v_num=4, val_loss=0.232, train_loss_step=0.0823, train_loss_epoch=0.0852]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|██████████| 22/22 [00:11<00:00,  1.84it/s, loss=0.0823, v_num=4, val_loss=0.228, train_loss_step=0.0809, train_loss_epoch=0.0824]\n",
      "Epoch 22:  95%|█████████▌| 21/22 [00:11<00:00,  1.76it/s, loss=0.0803, v_num=4, val_loss=0.228, train_loss_step=0.0792, train_loss_epoch=0.0824]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|██████████| 22/22 [00:12<00:00,  1.83it/s, loss=0.0803, v_num=4, val_loss=0.232, train_loss_step=0.0778, train_loss_epoch=0.0804]\n",
      "Epoch 23:  95%|█████████▌| 21/22 [00:12<00:00,  1.71it/s, loss=0.0785, v_num=4, val_loss=0.232, train_loss_step=0.0786, train_loss_epoch=0.0804]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|██████████| 22/22 [00:12<00:00,  1.78it/s, loss=0.0785, v_num=4, val_loss=0.234, train_loss_step=0.0779, train_loss_epoch=0.0784]\n",
      "Epoch 24:  95%|█████████▌| 21/22 [00:12<00:00,  1.68it/s, loss=0.0771, v_num=4, val_loss=0.234, train_loss_step=0.0757, train_loss_epoch=0.0784]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|██████████| 22/22 [00:12<00:00,  1.76it/s, loss=0.0771, v_num=4, val_loss=0.236, train_loss_step=0.0747, train_loss_epoch=0.0771]\n",
      "Epoch 24: 100%|██████████| 22/22 [00:12<00:00,  1.75it/s, loss=0.0771, v_num=4, val_loss=0.236, train_loss_step=0.0747, train_loss_epoch=0.0771]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | SMAPE                           | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 277   \n",
      "3  | prescalers                         | ModuleDict                      | 240   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.6 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 18.5 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "255 K     Trainable params\n",
      "0         Non-trainable params\n",
      "255 K     Total params\n",
      "1.021     Total estimated model params size (MB)\n",
      "Epoch 0:   0%|          | 0/22 [00:00<?, ?it/s] /home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  95%|█████████▌| 21/22 [00:12<00:00,  1.70it/s, loss=0.484, v_num=5, val_loss=0.626, train_loss_step=0.378]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 22/22 [00:12<00:00,  1.77it/s, loss=0.484, v_num=5, val_loss=0.337, train_loss_step=0.345, train_loss_epoch=0.490]\n",
      "Epoch 1:  95%|█████████▌| 21/22 [00:12<00:00,  1.66it/s, loss=0.299, v_num=5, val_loss=0.337, train_loss_step=0.290, train_loss_epoch=0.490]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 22/22 [00:12<00:00,  1.73it/s, loss=0.299, v_num=5, val_loss=0.290, train_loss_step=0.263, train_loss_epoch=0.301]\n",
      "Epoch 2:  95%|█████████▌| 21/22 [00:12<00:00,  1.67it/s, loss=0.251, v_num=5, val_loss=0.290, train_loss_step=0.242, train_loss_epoch=0.301]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 22/22 [00:12<00:00,  1.74it/s, loss=0.251, v_num=5, val_loss=0.280, train_loss_step=0.242, train_loss_epoch=0.251]\n",
      "Epoch 3:  95%|█████████▌| 21/22 [00:11<00:00,  1.76it/s, loss=0.235, v_num=5, val_loss=0.280, train_loss_step=0.231, train_loss_epoch=0.251]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 22/22 [00:11<00:00,  1.84it/s, loss=0.235, v_num=5, val_loss=0.220, train_loss_step=0.221, train_loss_epoch=0.235]\n",
      "Epoch 4:  95%|█████████▌| 21/22 [00:11<00:00,  1.83it/s, loss=0.219, v_num=5, val_loss=0.220, train_loss_step=0.216, train_loss_epoch=0.235]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 22/22 [00:11<00:00,  1.90it/s, loss=0.219, v_num=5, val_loss=0.223, train_loss_step=0.220, train_loss_epoch=0.219]\n",
      "Epoch 5:  95%|█████████▌| 21/22 [00:12<00:00,  1.75it/s, loss=0.195, v_num=5, val_loss=0.223, train_loss_step=0.197, train_loss_epoch=0.219]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 22/22 [00:12<00:00,  1.82it/s, loss=0.195, v_num=5, val_loss=0.208, train_loss_step=0.195, train_loss_epoch=0.196]\n",
      "Epoch 6:  95%|█████████▌| 21/22 [00:12<00:00,  1.70it/s, loss=0.191, v_num=5, val_loss=0.208, train_loss_step=0.192, train_loss_epoch=0.196]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 22/22 [00:12<00:00,  1.77it/s, loss=0.191, v_num=5, val_loss=0.225, train_loss_step=0.179, train_loss_epoch=0.190]\n",
      "Epoch 7:  95%|█████████▌| 21/22 [00:11<00:00,  1.77it/s, loss=0.191, v_num=5, val_loss=0.225, train_loss_step=0.193, train_loss_epoch=0.190]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 22/22 [00:11<00:00,  1.84it/s, loss=0.191, v_num=5, val_loss=0.213, train_loss_step=0.173, train_loss_epoch=0.191]\n",
      "Epoch 8:  95%|█████████▌| 21/22 [00:12<00:00,  1.70it/s, loss=0.173, v_num=5, val_loss=0.213, train_loss_step=0.160, train_loss_epoch=0.191]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 22/22 [00:12<00:00,  1.77it/s, loss=0.173, v_num=5, val_loss=0.207, train_loss_step=0.172, train_loss_epoch=0.173]\n",
      "Epoch 9:  95%|█████████▌| 21/22 [00:12<00:00,  1.70it/s, loss=0.157, v_num=5, val_loss=0.207, train_loss_step=0.150, train_loss_epoch=0.173]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 22/22 [00:12<00:00,  1.77it/s, loss=0.157, v_num=5, val_loss=0.202, train_loss_step=0.146, train_loss_epoch=0.158]\n",
      "Epoch 10:  59%|█████▉    | 13/22 [00:07<00:05,  1.73it/s, loss=0.155, v_num=5, val_loss=0.202, train_loss_step=0.157, train_loss_epoch=0.158]"
     ]
    }
   ],
   "source": [
    "for ___ in range(repeat):\n",
    "    for idx in range(len(data_bag)):\n",
    "\n",
    "        data = data_bag[idx]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        max_prediction_length = 168\n",
    "        max_encoder_length = 336\n",
    "        training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "        training = TimeSeriesDataSet(\n",
    "            data[lambda x: x.time_idx <= training_cutoff],\n",
    "            time_idx=\"time_idx\",\n",
    "            target=\"kWH\",\n",
    "            group_ids=[\"num\"],\n",
    "            min_encoder_length=max_encoder_length,\n",
    "            max_encoder_length=max_encoder_length,\n",
    "            min_prediction_length=max_prediction_length,\n",
    "            max_prediction_length=max_prediction_length,\n",
    "            static_categoricals=[\"num\", \"energy_group\"],\n",
    "            static_reals=[\"non_electric_aircondition\", \"sunlight\"],\n",
    "            time_varying_known_categoricals=[\"Week\", \"day_of_Week\", \"day_of_month\", \"day\", \"24Hour\", \"holiday\", \"Weekend\", \"hour_cat\"],\n",
    "            time_varying_known_reals=[\"C\", \"m/s\", \"wet\", \"mm\", \"hr\", \"time_idx\", \"perceived_temperature\", \"discomfort_index\"],\n",
    "            time_varying_unknown_categoricals=[],\n",
    "            time_varying_unknown_reals=[\"kWH\"],\n",
    "            add_relative_time_idx=True,\n",
    "            add_target_scales=True,\n",
    "            add_encoder_length=True,\n",
    "        )\n",
    "\n",
    "        # create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "        # for each series\n",
    "        validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "        # create dataloaders for model\n",
    "        batch_size = 64  # set this between 32 to 128\n",
    "        train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "        val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "        # configure network and trainer\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "        lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "        logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=500,\n",
    "            gpus=1,\n",
    "            weights_summary=\"top\",\n",
    "            gradient_clip_val=0.14,\n",
    "            limit_train_batches=30, \n",
    "            callbacks=[lr_logger, early_stop_callback],\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "\n",
    "        tft = TemporalFusionTransformer.from_dataset(\n",
    "            training,\n",
    "            learning_rate=0.03,\n",
    "            hidden_size=64,\n",
    "            lstm_layers = 1,\n",
    "            attention_head_size=4,\n",
    "            dropout=0.15,\n",
    "            hidden_continuous_size=8,\n",
    "            output_size=1,\n",
    "            loss=SMAPE(),\n",
    "            log_interval=0,\n",
    "            reduce_on_plateau_patience=4,\n",
    "        )\n",
    "\n",
    "        # fit network\n",
    "        trainer.fit(\n",
    "            tft,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloaders=val_dataloader,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd02625b230b0936c03a7a859dabe4a10a429313c958b60e79e367b6eadd1f82f0b",
   "display_name": "Python 3.9.1 64-bit ('torch1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}