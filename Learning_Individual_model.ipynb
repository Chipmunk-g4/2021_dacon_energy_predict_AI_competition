{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Refined_Data/Grouped_Data/Input_Data2.csv\"\n",
    "\n",
    "building_num = [[34]]\n",
    "\n",
    "repeat = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, parse_dates = [\"date_time\"])\n",
    "\n",
    "data['num']     =   data['num'].apply(str)\n",
    "data['Week']    =   data['Week'].apply(str)\n",
    "data['24Hour']  =   data['24Hour'].apply(str)\n",
    "data['holiday'] =   data['holiday'].apply(str)\n",
    "data['Weekend'] =   data['Weekend'].apply(str)\n",
    "data['energy_group'] = data['energy_group'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bag = [data.loc[data[\"num\"] == str(building_num[i])].copy() for i in range(len(building_num))]\n",
    "\n",
    "data_bag = []\n",
    "for buildings in building_num:\n",
    "    temp = data.loc[data[\"num\"] == str(buildings[0])].copy()\n",
    "    for num_idx in range(1, len(buildings)):\n",
    "        temp = pd.concat([temp, data.loc[data[\"num\"] == str(buildings[num_idx])].copy()])\n",
    "    data_bag.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "rch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  97%|█████████▋| 30/31 [00:08<00:00,  3.41it/s, loss=71.6, v_num=7, val_loss=176.0, train_loss_step=57.30]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 31/31 [00:08<00:00,  3.50it/s, loss=71.6, v_num=7, val_loss=37.80, train_loss_step=63.90, train_loss_epoch=94.20]\n",
      "Epoch 1:  97%|█████████▋| 30/31 [00:08<00:00,  3.56it/s, loss=51.9, v_num=7, val_loss=37.80, train_loss_step=51.30, train_loss_epoch=94.20]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 31/31 [00:08<00:00,  3.65it/s, loss=51.9, v_num=7, val_loss=29.40, train_loss_step=47.50, train_loss_epoch=54.90]\n",
      "Epoch 2:  97%|█████████▋| 30/31 [00:08<00:00,  3.46it/s, loss=34.5, v_num=7, val_loss=29.40, train_loss_step=30.60, train_loss_epoch=54.90]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 31/31 [00:08<00:00,  3.54it/s, loss=34.5, v_num=7, val_loss=26.90, train_loss_step=32.10, train_loss_epoch=37.40]\n",
      "Epoch 3:  97%|█████████▋| 30/31 [00:08<00:00,  3.44it/s, loss=26.8, v_num=7, val_loss=26.90, train_loss_step=24.00, train_loss_epoch=37.40]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 31/31 [00:08<00:00,  3.53it/s, loss=26.8, v_num=7, val_loss=30.80, train_loss_step=25.70, train_loss_epoch=27.40]\n",
      "Epoch 4:  97%|█████████▋| 30/31 [00:08<00:00,  3.43it/s, loss=21.7, v_num=7, val_loss=30.80, train_loss_step=22.60, train_loss_epoch=27.40]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 31/31 [00:08<00:00,  3.51it/s, loss=21.7, v_num=7, val_loss=28.60, train_loss_step=21.70, train_loss_epoch=22.30]\n",
      "Epoch 5:  97%|█████████▋| 30/31 [00:08<00:00,  3.61it/s, loss=21, v_num=7, val_loss=28.60, train_loss_step=17.80, train_loss_epoch=22.30]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 31/31 [00:08<00:00,  3.70it/s, loss=21, v_num=7, val_loss=22.20, train_loss_step=19.90, train_loss_epoch=21.10]\n",
      "Epoch 6:  97%|█████████▋| 30/31 [00:08<00:00,  3.59it/s, loss=20.4, v_num=7, val_loss=22.20, train_loss_step=21.30, train_loss_epoch=21.10]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 31/31 [00:08<00:00,  3.68it/s, loss=20.4, v_num=7, val_loss=23.20, train_loss_step=20.00, train_loss_epoch=19.90]\n",
      "Epoch 7:  97%|█████████▋| 30/31 [00:08<00:00,  3.50it/s, loss=16.7, v_num=7, val_loss=23.20, train_loss_step=17.90, train_loss_epoch=19.90]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 31/31 [00:08<00:00,  3.59it/s, loss=16.7, v_num=7, val_loss=35.90, train_loss_step=16.70, train_loss_epoch=17.60]\n",
      "Epoch 8:  97%|█████████▋| 30/31 [00:08<00:00,  3.50it/s, loss=16, v_num=7, val_loss=35.90, train_loss_step=16.70, train_loss_epoch=17.60]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 31/31 [00:08<00:00,  3.60it/s, loss=16, v_num=7, val_loss=42.90, train_loss_step=15.30, train_loss_epoch=16.20]\n",
      "Epoch 9:  97%|█████████▋| 30/31 [00:08<00:00,  3.56it/s, loss=14.6, v_num=7, val_loss=42.90, train_loss_step=17.00, train_loss_epoch=16.20]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 31/31 [00:08<00:00,  3.64it/s, loss=14.6, v_num=7, val_loss=28.20, train_loss_step=13.20, train_loss_epoch=14.80]\n",
      "Epoch 10:  97%|█████████▋| 30/31 [00:08<00:00,  3.53it/s, loss=15.1, v_num=7, val_loss=28.20, train_loss_step=16.50, train_loss_epoch=14.80]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 31/31 [00:08<00:00,  3.62it/s, loss=15.1, v_num=7, val_loss=25.20, train_loss_step=13.40, train_loss_epoch=14.60]\n",
      "Epoch 11:  97%|█████████▋| 30/31 [00:08<00:00,  3.52it/s, loss=11.9, v_num=7, val_loss=25.20, train_loss_step=10.80, train_loss_epoch=14.60]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 31/31 [00:08<00:00,  3.61it/s, loss=11.9, v_num=7, val_loss=38.40, train_loss_step=13.90, train_loss_epoch=12.20]\n",
      "Epoch 12:  97%|█████████▋| 30/31 [00:08<00:00,  3.53it/s, loss=11.4, v_num=7, val_loss=38.40, train_loss_step=11.80, train_loss_epoch=12.20]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 31/31 [00:08<00:00,  3.62it/s, loss=11.4, v_num=7, val_loss=38.30, train_loss_step=12.40, train_loss_epoch=11.50]\n",
      "Epoch 13:  97%|█████████▋| 30/31 [00:08<00:00,  3.60it/s, loss=10.9, v_num=7, val_loss=38.30, train_loss_step=11.80, train_loss_epoch=11.50]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 31/31 [00:08<00:00,  3.68it/s, loss=10.9, v_num=7, val_loss=30.00, train_loss_step=11.00, train_loss_epoch=11.00]\n",
      "Epoch 14:  97%|█████████▋| 30/31 [00:08<00:00,  3.51it/s, loss=10.4, v_num=7, val_loss=30.00, train_loss_step=9.870, train_loss_epoch=11.00]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 31/31 [00:08<00:00,  3.60it/s, loss=10.4, v_num=7, val_loss=30.70, train_loss_step=9.740, train_loss_epoch=10.50]\n",
      "Epoch 15:  97%|█████████▋| 30/31 [00:08<00:00,  3.49it/s, loss=10.5, v_num=7, val_loss=30.70, train_loss_step=10.80, train_loss_epoch=10.50]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 31/31 [00:08<00:00,  3.57it/s, loss=10.5, v_num=7, val_loss=32.20, train_loss_step=10.10, train_loss_epoch=10.40]\n",
      "Epoch 15: 100%|██████████| 31/31 [00:08<00:00,  3.57it/s, loss=10.5, v_num=7, val_loss=32.20, train_loss_step=10.10, train_loss_epoch=10.40]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 257   \n",
      "3  | prescalers                         | ModuleDict                      | 240   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.0 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 18.1 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 66.6 K\n",
      "12 | lstm_decoder                       | LSTM                            | 66.6 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 455   \n",
      "----------------------------------------------------------------------------------------\n",
      "321 K     Trainable params\n",
      "0         Non-trainable params\n",
      "321 K     Total params\n",
      "1.285     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]/home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|          | 0/31 [00:00<?, ?it/s] /home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  97%|█████████▋| 30/31 [00:08<00:00,  3.50it/s, loss=69.5, v_num=8, val_loss=173.0, train_loss_step=61.90]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 31/31 [00:08<00:00,  3.58it/s, loss=69.5, v_num=8, val_loss=42.80, train_loss_step=60.60, train_loss_epoch=94.20]\n",
      "Epoch 1:  97%|█████████▋| 30/31 [00:08<00:00,  3.54it/s, loss=51, v_num=8, val_loss=42.80, train_loss_step=43.30, train_loss_epoch=94.20]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 31/31 [00:08<00:00,  3.63it/s, loss=51, v_num=8, val_loss=30.90, train_loss_step=41.80, train_loss_epoch=53.30]\n",
      "Epoch 2:  97%|█████████▋| 30/31 [00:08<00:00,  3.59it/s, loss=32.7, v_num=8, val_loss=30.90, train_loss_step=27.40, train_loss_epoch=53.30]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 31/31 [00:08<00:00,  3.68it/s, loss=32.7, v_num=8, val_loss=25.90, train_loss_step=30.20, train_loss_epoch=35.40]\n",
      "Epoch 3:  97%|█████████▋| 30/31 [00:08<00:00,  3.58it/s, loss=28, v_num=8, val_loss=25.90, train_loss_step=26.20, train_loss_epoch=35.40]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 31/31 [00:08<00:00,  3.68it/s, loss=28, v_num=8, val_loss=24.50, train_loss_step=25.90, train_loss_epoch=28.40]\n",
      "Epoch 4:  97%|█████████▋| 30/31 [00:08<00:00,  3.49it/s, loss=24.8, v_num=8, val_loss=24.50, train_loss_step=26.30, train_loss_epoch=28.40]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 31/31 [00:08<00:00,  3.57it/s, loss=24.8, v_num=8, val_loss=29.10, train_loss_step=24.10, train_loss_epoch=25.30]\n",
      "Epoch 5:  97%|█████████▋| 30/31 [00:08<00:00,  3.48it/s, loss=20.5, v_num=8, val_loss=29.10, train_loss_step=19.80, train_loss_epoch=25.30]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 31/31 [00:08<00:00,  3.56it/s, loss=20.5, v_num=8, val_loss=42.10, train_loss_step=17.30, train_loss_epoch=21.00]\n",
      "Epoch 6:  97%|█████████▋| 30/31 [00:08<00:00,  3.45it/s, loss=17.8, v_num=8, val_loss=42.10, train_loss_step=19.20, train_loss_epoch=21.00]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 31/31 [00:08<00:00,  3.53it/s, loss=17.8, v_num=8, val_loss=53.90, train_loss_step=19.50, train_loss_epoch=18.30]\n",
      "Epoch 7:  97%|█████████▋| 30/31 [00:08<00:00,  3.59it/s, loss=17.9, v_num=8, val_loss=53.90, train_loss_step=16.20, train_loss_epoch=18.30]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 31/31 [00:08<00:00,  3.68it/s, loss=17.9, v_num=8, val_loss=43.00, train_loss_step=15.90, train_loss_epoch=18.40]\n",
      "Epoch 8:  97%|█████████▋| 30/31 [00:08<00:00,  3.45it/s, loss=17.5, v_num=8, val_loss=43.00, train_loss_step=16.70, train_loss_epoch=18.40]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 31/31 [00:08<00:00,  3.54it/s, loss=17.5, v_num=8, val_loss=37.20, train_loss_step=17.60, train_loss_epoch=17.00]\n",
      "Epoch 9:  97%|█████████▋| 30/31 [00:08<00:00,  3.36it/s, loss=14, v_num=8, val_loss=37.20, train_loss_step=13.80, train_loss_epoch=17.00]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 31/31 [00:08<00:00,  3.44it/s, loss=14, v_num=8, val_loss=51.60, train_loss_step=13.30, train_loss_epoch=14.30]\n",
      "Epoch 10:  97%|█████████▋| 30/31 [00:08<00:00,  3.55it/s, loss=13.3, v_num=8, val_loss=51.60, train_loss_step=14.50, train_loss_epoch=14.30]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 31/31 [00:08<00:00,  3.64it/s, loss=13.3, v_num=8, val_loss=45.30, train_loss_step=10.60, train_loss_epoch=13.30]\n",
      "Epoch 11:  97%|█████████▋| 30/31 [00:08<00:00,  3.56it/s, loss=12.9, v_num=8, val_loss=45.30, train_loss_step=12.60, train_loss_epoch=13.30]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 31/31 [00:08<00:00,  3.64it/s, loss=12.9, v_num=8, val_loss=44.70, train_loss_step=13.00, train_loss_epoch=12.90]\n",
      "Epoch 12:  97%|█████████▋| 30/31 [00:08<00:00,  3.51it/s, loss=12.3, v_num=8, val_loss=44.70, train_loss_step=11.70, train_loss_epoch=12.90]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 31/31 [00:08<00:00,  3.60it/s, loss=12.3, v_num=8, val_loss=40.50, train_loss_step=11.70, train_loss_epoch=12.40]\n",
      "Epoch 13:  97%|█████████▋| 30/31 [00:08<00:00,  3.54it/s, loss=12.3, v_num=8, val_loss=40.50, train_loss_step=12.90, train_loss_epoch=12.40]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 31/31 [00:08<00:00,  3.63it/s, loss=12.3, v_num=8, val_loss=36.40, train_loss_step=11.50, train_loss_epoch=12.10]\n",
      "Epoch 13: 100%|██████████| 31/31 [00:08<00:00,  3.63it/s, loss=12.3, v_num=8, val_loss=36.40, train_loss_step=11.50, train_loss_epoch=12.10]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 257   \n",
      "3  | prescalers                         | ModuleDict                      | 240   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 9.0 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.0 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 18.1 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 66.6 K\n",
      "12 | lstm_decoder                       | LSTM                            | 66.6 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 455   \n",
      "----------------------------------------------------------------------------------------\n",
      "321 K     Trainable params\n",
      "0         Non-trainable params\n",
      "321 K     Total params\n",
      "1.285     Total estimated model params size (MB)\n",
      "Epoch 0:   0%|          | 0/31 [00:00<?, ?it/s] /home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/joeunchan/anaconda3/envs/torch1/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  97%|█████████▋| 30/31 [00:08<00:00,  3.38it/s, loss=70.8, v_num=9, val_loss=134.0, train_loss_step=62.00]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 31/31 [00:08<00:00,  3.46it/s, loss=70.8, v_num=9, val_loss=48.70, train_loss_step=66.70, train_loss_epoch=97.90]\n",
      "Epoch 1:  97%|█████████▋| 30/31 [00:08<00:00,  3.55it/s, loss=56, v_num=9, val_loss=48.70, train_loss_step=50.40, train_loss_epoch=97.90]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 31/31 [00:08<00:00,  3.63it/s, loss=56, v_num=9, val_loss=37.60, train_loss_step=53.10, train_loss_epoch=57.40]\n",
      "Epoch 2:  97%|█████████▋| 30/31 [00:08<00:00,  3.49it/s, loss=41.9, v_num=9, val_loss=37.60, train_loss_step=38.40, train_loss_epoch=57.40]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 31/31 [00:08<00:00,  3.59it/s, loss=41.9, v_num=9, val_loss=28.50, train_loss_step=34.40, train_loss_epoch=44.30]\n",
      "Epoch 3:  97%|█████████▋| 30/31 [00:08<00:00,  3.44it/s, loss=28.9, v_num=9, val_loss=28.50, train_loss_step=23.60, train_loss_epoch=44.30]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 31/31 [00:08<00:00,  3.53it/s, loss=28.9, v_num=9, val_loss=22.20, train_loss_step=24.20, train_loss_epoch=30.50]\n",
      "Epoch 4:  97%|█████████▋| 30/31 [00:09<00:00,  3.33it/s, loss=23, v_num=9, val_loss=22.20, train_loss_step=20.60, train_loss_epoch=30.50]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 31/31 [00:09<00:00,  3.42it/s, loss=23, v_num=9, val_loss=26.00, train_loss_step=20.70, train_loss_epoch=23.70]\n",
      "Epoch 5:  97%|█████████▋| 30/31 [00:08<00:00,  3.44it/s, loss=18.9, v_num=9, val_loss=26.00, train_loss_step=19.50, train_loss_epoch=23.70]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 31/31 [00:08<00:00,  3.52it/s, loss=18.9, v_num=9, val_loss=26.80, train_loss_step=17.10, train_loss_epoch=19.70]\n",
      "Epoch 6:  97%|█████████▋| 30/31 [00:08<00:00,  3.57it/s, loss=17.7, v_num=9, val_loss=26.80, train_loss_step=16.80, train_loss_epoch=19.70]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 31/31 [00:08<00:00,  3.65it/s, loss=17.7, v_num=9, val_loss=20.20, train_loss_step=18.60, train_loss_epoch=17.80]\n",
      "Epoch 7:  97%|█████████▋| 30/31 [00:08<00:00,  3.59it/s, loss=16.2, v_num=9, val_loss=20.20, train_loss_step=16.30, train_loss_epoch=17.80]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 31/31 [00:08<00:00,  3.67it/s, loss=16.2, v_num=9, val_loss=21.20, train_loss_step=14.00, train_loss_epoch=16.00]\n",
      "Epoch 8:  97%|█████████▋| 30/31 [00:08<00:00,  3.59it/s, loss=15.9, v_num=9, val_loss=21.20, train_loss_step=15.30, train_loss_epoch=16.00]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 31/31 [00:08<00:00,  3.67it/s, loss=15.9, v_num=9, val_loss=25.90, train_loss_step=18.80, train_loss_epoch=15.70]\n",
      "Epoch 9:  97%|█████████▋| 30/31 [00:08<00:00,  3.51it/s, loss=14.1, v_num=9, val_loss=25.90, train_loss_step=17.00, train_loss_epoch=15.70]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 31/31 [00:08<00:00,  3.60it/s, loss=14.1, v_num=9, val_loss=29.30, train_loss_step=14.30, train_loss_epoch=14.20]\n",
      "Epoch 10:   6%|▋         | 2/31 [00:00<00:08,  3.34it/s, loss=13.7, v_num=9, val_loss=29.30, train_loss_step=11.60, train_loss_epoch=14.20]"
     ]
    }
   ],
   "source": [
    "for ___ in range(repeat):\n",
    "    for idx in range(len(data_bag)):\n",
    "\n",
    "        data = data_bag[idx]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        max_prediction_length = 24\n",
    "        max_encoder_length = 168\n",
    "        training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "        training = TimeSeriesDataSet(\n",
    "            data[lambda x: x.time_idx <= training_cutoff],\n",
    "            time_idx=\"time_idx\",\n",
    "            target=\"kWH\",\n",
    "            group_ids=[\"num\"],\n",
    "            min_encoder_length=max_encoder_length//2,\n",
    "            max_encoder_length=max_encoder_length,\n",
    "            min_prediction_length=3,\n",
    "            max_prediction_length=max_prediction_length,\n",
    "            static_categoricals=[\"num\", \"energy_group\"],\n",
    "            static_reals=[\"non_electric_aircondition\", \"sunlight\"],\n",
    "            time_varying_known_categoricals=[\"Week\", \"24Hour\", \"holiday\", \"Weekend\"],\n",
    "            time_varying_known_reals=[\"C\", \"m/s\", \"wet\", \"mm\", \"hr\", \"time_idx\", \"perceived_temperature\", \"discomfort_index\"],\n",
    "            time_varying_unknown_categoricals=[],\n",
    "            time_varying_unknown_reals=[\"kWH\"],\n",
    "            add_relative_time_idx=True,\n",
    "            add_target_scales=True,\n",
    "            add_encoder_length=True,\n",
    "        )\n",
    "\n",
    "        # create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "        # for each series\n",
    "        validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "        # create dataloaders for model\n",
    "        batch_size = 64  # set this between 32 to 128\n",
    "        train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "        val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "        # configure network and trainer\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "        lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "        logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=500,\n",
    "            gpus=1,\n",
    "            weights_summary=\"top\",\n",
    "            gradient_clip_val=0.14,\n",
    "            limit_train_batches=30, \n",
    "            callbacks=[lr_logger, early_stop_callback],\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "\n",
    "        tft = TemporalFusionTransformer.from_dataset(\n",
    "            training,\n",
    "            learning_rate=0.03,\n",
    "            hidden_size=64,\n",
    "            lstm_layers = 2,\n",
    "            attention_head_size=4,\n",
    "            dropout=0.15,\n",
    "            hidden_continuous_size=8,\n",
    "            output_size=7,\n",
    "            loss=QuantileLoss(),\n",
    "            log_interval=0,\n",
    "            reduce_on_plateau_patience=4,\n",
    "        )\n",
    "\n",
    "        # fit network\n",
    "        trainer.fit(\n",
    "            tft,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloaders=val_dataloader,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd02625b230b0936c03a7a859dabe4a10a429313c958b60e79e367b6eadd1f82f0b",
   "display_name": "Python 3.9.1 64-bit ('torch1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}