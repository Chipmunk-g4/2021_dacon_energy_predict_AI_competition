for ___ in range(repeat):
    for idx in range(len(data_bag)):

        data = data_bag[idx]

        torch.cuda.empty_cache()

        max_prediction_length = 24
        max_encoder_length = 168
        training_cutoff = data["time_idx"].max() - max_prediction_length

        training = TimeSeriesDataSet(
            data[lambda x: x.time_idx <= training_cutoff],
            time_idx="time_idx",
            target="kWH",
            group_ids=["num"],
            min_encoder_length=max_encoder_length//2,
            max_encoder_length=max_encoder_length,
            min_prediction_length=3,
            max_prediction_length=max_prediction_length,
            static_categoricals=["num", "energy_group"],
            static_reals=["non_electric_aircondition", "sunlight"],
            time_varying_known_categoricals=["Week", "24Hour", "holiday", "Weekend"],
            time_varying_known_reals=["C", "m/s", "wet", "mm", "hr", "time_idx", "perceived_temperature", "discomfort_index"],
            time_varying_unknown_categoricals=[],
            time_varying_unknown_reals=["kWH"],
            add_relative_time_idx=True,
            add_target_scales=True,
            add_encoder_length=True,
        )

        # create validation set (predict=True) which means to predict the last max_prediction_length points in time
        # for each series
        validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)

        # create dataloaders for model
        batch_size = 64  # set this between 32 to 128
        train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)
        val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)

        # configure network and trainer
        early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=1e-4, patience=10, verbose=False, mode="min")
        lr_logger = LearningRateMonitor()  # log the learning rate
        logger = TensorBoardLogger("lightning_logs")  # logging results to a tensorboard

        trainer = pl.Trainer(
            max_epochs=500,
            gpus=1,
            weights_summary="top",
            gradient_clip_val=0.14,
            limit_train_batches=30, 
            callbacks=[lr_logger, early_stop_callback],
            logger=logger,
        )


        tft = TemporalFusionTransformer.from_dataset(
            training,
            learning_rate=0.03,
            hidden_size=64,
            lstm_layers = 2,
            attention_head_size=4,
            dropout=0.15,
            hidden_continuous_size=8,
            output_size=7,
            loss=QuantileLoss(),
            log_interval=0,
            reduce_on_plateau_patience=4,
        )

        # fit network
        trainer.fit(
            tft,
            train_dataloader=train_dataloader,
            val_dataloaders=val_dataloader,
        )